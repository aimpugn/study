# Concurrent and hardware

- [Concurrent and hardware](#concurrent-and-hardware)
    - [비동기와 동시성](#비동기와-동시성)
    - [저장장치와 비동기](#저장장치와-비동기)
    - [CPU -\> 저장장치 -\> 데이터가 메모리에 로드되는 과정](#cpu---저장장치---데이터가-메모리에-로드되는-과정)

## 비동기와 동시성

프로세스나 스레드가 어떤 작업을 시스템 콜로 위임하면 커널은 해당 작업을 장치 드라이버를 통해 하드웨어로 내려보내는데, 그 순간부터 *사용자 공간의 실행 흐름*은 그 작업을 직접 기다릴 필요가 없습니다.

디스크 컨트롤러는 [DMA(Direct Memory Access)](https://en.wikipedia.org/wiki/Direct_memory_access)로 데이터를 메모리에 실어 나르고, 전송이 끝나면 인터럽트를 발생시켜 커널에게 알립니다.

> 직접 메모리 액세스
>
> 특정 하드웨어 하위 시스템이 *중앙 처리 장치(CPU)와 독립적*으로 주 시스템메모리에 액세스할 수 있도록 하는 컴퓨터 시스템 기능입니다.

호출자 입장에서 보면 "요청 -> 곧바로 반환 -> 나중에 완료 통지"라는 세 단계가 펼쳐지는데, 이 패턴이 바로 비동기(asynchronous)입니다.
비동기는 *"내가 맡긴 일이 지금 즉시 끝나지 않더라도 그 자리에서 멈추지 않고 다른 일(혹은 아무 일도 안 하는 '이벤트 루프 대기')로 넘어갈 수 있다"라는 제어흐름의 특성*을 설명합니다.

반면 동시성(concurrency)  시스템이 *논리적으로 여러 작업을 한 덩어리처럼 앞뒤로 뒤섞어 실행한다*는 더 넓은 개념입니다.
하나의 CPU 코어에서 시분할로 작업을 교대하든, 여럿의 코어에서 물리적으로 병렬로 돌리든 모두 동시성 모델로 해석할 수 있습니다.
그러므로 비동기 호출을 쓴다고 해서 자동으로 작업이 여럿 동시에 진행되는 것은 아니지만, 비동기는 동시성을 설계할 때 필수적인 빌딩 블록이 됩니다.

가령 이벤트 루프 기반 서버처럼 단일 스레드 구조라 해도, 그 한 스레드는 수많은 클라이언트 요청을 비동기(asynchronous) I/O 덕분에 끊김 없이 번갈아 처리하며 "동시성"을 달성합니다.

즉, 비동기는 '기다리지 않을 권리'를 제공하고, 동시성은 그 권리를 포함해 '여러 흐름을 조직하는 방법'을 의미합니다.

## 저장장치와 비동기

현대의 SATA SSD는 [NCQ(Native Command Queuing)](https://en.wikipedia.org/wiki/Native_Command_Queuing)로 최대 32개의 명령을, [NVMe SSD](https://blog.westerndigital.com/nvme-queues-explained/)는 [수천 개의 커맨드를 큐에 쌓아 둔 채 내부 컨트롤러가 병렬 채널로 처리](https://www.kingston.com/en/ssd/what-is-nvme-ssd-technology#:~:text=While%20NVMe%20has%2064K%20command%20queues%20and%20can%20send%2064K%20commands%20per%20queue%2C%20AHCI%20only%20has%20one%20command%20queue%20and%20can%20only%20send%20thirty%2Dtwo%20commands%20per%20queue)한다고 합니다.

> NCQ와 TCQ
>
> NCQ 전에는 Tagged Command Queuing(TCQ)라는 기능이 있었습니다.
> 하지만 TCQ는 드라이브보다는 호스트 어댑터 로직이 주도권을 갖고 있었고, CPU가 태그를 관리해야 했습니다.
> 즉, 장치 자체가 스스로 여러 명령을 재배열하지 못했습니다.
>
> NCQ는 이와 달리 호스트가 명령을 밀어 넣고 다른 일을 하더라도 장치가 알아서 순서를 재조정하는, 끝까지 하드웨어가 주도하는 방식입니다.
> 즉 드라이브 펌웨어 수준에서 네이티브로 동작하는 큐라는 의미입니다.

호스트가 한 요청을 보낸 직후 곧바로 다음 요청을 큐에 추가할 수 있고, SSD 내부 펌웨어는 die 여러 개를 시간적으로 오버랩하여 병렬 구동합니다.

> 실리콘 웨이퍼에서 잘려 나온 개별 칩 하나를 [플래시 die](https://www.atpinc.com/tw/blog/what-is-nand-die-stacking)라고 부릅니다.
> - 한 die 안에는 페이지들이 모여 블록을 이루고,
> - 블록들이 다시 여러 개의 plane으로 구획되며,
> - 이 구조 전체를 구동하기 위한 주변 회로가 가장자리 링처럼 둘러싸여 있습니다.
>
> $$
> Page < block < Plane < Die
> $$

읽기, 쓰기, 지우기 같은 NAND(NOT AND) 명령은 내부적으로 '어드레싱 -> 데이터 버퍼링 -> 셀 전압 세팅 -> 확인'을 포함하며 마이크로초 단위의 시간이 소요됩니다.

> [플래시 메모리 분류: NAND와 NOR](https://computing-jhson.tistory.com/115)
>
> - NAND 플래시:
>
>   반도체 셀이 직렬로 배열된 구조입니다.
>   만약 셀 하나를 읽고 싶다면, 직렬로 연결된 나머지 모든 셀들을 ON 시켜야 해당 셀의 내용을 읽을 수 있습니다.
>   따라서 셀 하나를 읽고 쓰는 데 시간이 오래 걸립니다.
>   하지만 NOR에 비해 집적도가 높습니다.
>
> - NOR 플래시:
>
>   반도체 셀이 병렬로 배열된 구조입니다.
>   만약 셀 하나를 읽고 싶다면, 해당 셀의 word line에만 전압을 걸어주면 해당 셀을 읽고 쓸 수 있습니다.
>   따라서 셀 하나를 읽고 쓰는 데 시간이 빠릅니다.
>   또한 셀마다 source line과 직접 연결되어야 하므로 셀의 크기가 NAND보다 상대적으로 크고,
>   source line들도 셀 개수만큼 많이 설치해야 해서, NAND보다 더 적은 셀들을 배치하게 되어 집적도가 떨어집니다.

컨트롤러 펌웨어는 명령 하나가 특정 다이에서 전압을 충전하는 동안, 이미 다른 다이에게는 다음 명령의 어드레스를 보내두고, 또 다른 다이에서는 완료된 데이터 버퍼를 DMA로 끌어오게 하는 식으로 파이프라이닝을 교차 배치합니다.
이러한 기법을 interleaving 또는 way pipelining이라고 합니다.
- [메모리 인터리빙 (Memory Interleaving)](https://blog.skby.net/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%9D%B8%ED%84%B0%EB%A6%AC%EB%B9%99-memory-interleaving/)
- [개발자를 위한 SSD (Coding for SSD) - Part 4 : 고급 기능과 내부 병렬 처리](https://tech.kakao.com/posts/329#ssd%EC%9D%98-%EB%82%B4%EB%B6%80-%EB%B3%91%EB%A0%AC-%EC%B2%98%EB%A6%AC)
- [Solid State Drive Primer # 8 - Controller Architecture - Channels and Banks](https://www.cactus-tech.com/resources/blog/details/solid-state-drive-primer-8-controller-architecture-channels-and-banks/)

어떤 다이에 언제 명령을 넣을지는 컨트롤러가 선택합니다.
펌웨어는 호스트가 큐에 던져 놓은 논리 주소를 자체 FTL 매핑 테이블로 물리 페이지에 풀어낸 뒤, 채널별/다이별 가용 상태를 살펴가며 스케쥴링한다고 합니다.
만약 컨트롤러가 내부 자원을 고르게 활용하지 못하면 일부 다이만 바빠지고, 나머지는 일을 하지 않으므로 IOPS가 급격히 떨어진다고 합니다.

이러한 하드웨어 레벨의 병렬성 덕분에, 커널이나 런타임이 비동기로 I/O를 날려 주면 CPU는 다른 일을 지속하면서도 디스크는 끊임없이 파이프라인을 채워 놓을 수 있습니다.
호스트가 NVMe 큐에 연속해서 요청을 밀어 넣어도 SSD 내부에서는 그 순서를 그대로 따르지 않고,
컨트롤러는 지연 시간이 긴 플래시 물리 동작을 여러 다이에 분산시키고, 동시에 진행 가능한 단계끼리는 병렬로 늘어뜨려 파이프라인 공백을 최소화하는 등 지연 구간을 줄입니다.
이러한 동적 스케줄러 덕분에 높은 수치의 IOPS가 가능하다고 합니다.

## CPU -> 저장장치 -> 데이터가 메모리에 로드되는 과정

초창기 컴퓨터에서 CPU는 I/O 버스를 직접 제어하며 한 바이트씩 자료를 주고받았습니다.
이로 인해서 "CPU가 연산 대신 복사에 발목이 잡힌다"는 병목이 드러났습니다.
이에 대해 1950년대 IBM 'Channel I/O'가 "전송은 별도 하드웨어에게 맡기고 CPU는 계산에 집중"하는는 해법을 제시했다고 합니다.

이 아이디어는 1979년 AMD 8237(인텔 8237 라이선스) 같은 범용 DMA(Direct Memory Access) 컨트롤러로 자리 잡았습니다.

> 8237은 시스템 버스를 잠시 '마스터'로 빼앗아 메모리 주소, 제어 신호를 직접 발생시켰으므로, CPU는 버스 중재 회선만 살짝 넘겨주고 다른 일을 계속할 수 있었다고 합니다.

그리고 오늘날 SoC에는 수십 개 DMA 채널이 온칩으로 통합됩니다.

결국 핵심은 같습니다:
1. CPU가 "여기부터 얼마만큼 보내라"는 버스 소유권을 넘깁니다.
2. DMA 엔진이 메모리 주소선, 제어선을 '마스터' 자격으로 구동해 데이터를 실어 나릅니다.
3. 끝나면 인터럽트로 CPU에게 결과만 알립니다.

SATA AHCI와 NVMe 모두 '장치 스스로 버스 마스터가 되어 호스트 RAM을 읽고 쓰는' 방식을 채택합니다.
DMA 전송이 끝나면 장치는 인터럽트나 MSI-X 인터럽트를 날리면, 커널은 그 인터럽트 벡터에 대응하는 Completion Queue 엔트리를 확인합니다.
이때 CPU가 깨어나 사용자 스레드에게 Future 완료 알림을 전달하는데,
이는 전형적인 'async -> callback or await' 패턴입니다.

SSD 컨트롤러에는 수 개에서 수십 개의 채널이 병렬로 배선돼 있습니다.
각 채널은 전용 상태 레지스터, 타이머, 제어 회로를 품은 유한 상태 기계(FSM)로 동작합니다.

```sh
Idle ─► Command_Latched ─► Busy(NAND동작) ─► Data_Transfer ─► Idle
        ▲                               │
        └──────── Ready/Busy pin ───────┘
```

채널 A가 'Busy' 상태에서 수백 µs짜리 블록 지우기를 수행하는 동안, B, C 등 다른 채널은 'Idle -> Command_Latched'로 넘어가서 짧은 페이지 읽기를 여러 번 끝낼 수 있습니다.
이렇게 '채널별 FSM'이 독립되어 있기에 어떤 채널의 지연이 다른 채널 처리량을 막지 않습니다.

> 여기서 FSM은 "유한 상태 기계(Finite-State Machine)"를 의미합니다.
>
> 가장 단순하게는 'Idle -> Command Latched -> Busy (NAND 내부에서 실제 읽기, 쓰기, 삭제 펄스를 가하는 구간)-> Data Transfer -> Idle' 사이클을 돌며 어떤 상태에서 다른 상태로 넘어갈지 다음과 같은 요소들에 의해 결정된다고 합니다.
> - 레지스터 신호
> - 내부 타이머
> - 다이로부터 되돌아오는 Ready/Busy 핀
>
> 즉, 플래시 채널을 독립된 FSM처럼 구동한다는 것은 채널이 *자기 고유의 상태 레지스터와 타이머*를 갖고 호스트 명령을 순차 논리 회로로 소화한다는 것을 의미합니다.
> 이런 구조 덕분에 *여러 채널이 물리적으로 동시에 다른 상태*에 머무를 수 있습니다.

채널마다 "읽기 활성화 -> 데이터 버퍼링 -> ECC 검증"이 돌아가는 동안, 다른 채널은 이미 다음 페이지를 요구하거나 쓰기 프로그램 패스에 들어가는 방식입니다.
플래시 안쪽에서는 word line 여러 개를 동시에 미세 전압으로 캐싱하거나, 하나의 On-die SRAM 버퍼가 다음 페이지를 처리하기 위해 대기합니다.

> On-die SRAM 버퍼:
>
> 여기서 다이는 웨이퍼에서 잘려 나온 실리콘 칩 하나를 의미합니다.
> 여러 plane(블록 묶음)을 포함합니다.
>
> 이 On-die SRAM은 die 내부에 자리한 임시 버퍼입니다.
> 셀 어레이에서 끌어 올린 수 십 KiB 페이지, ECC 패리티, 메타데이터를 잠시 보관합니다.
>
> 컨트롤러는 한 die가 전압을 프로그래밍하는 동안, 다른 die에서 이미 다음 읽기 주소를 대기시키고, 또 다른 die의 On-die SRAM에 담긴 데이터를 채널 버스로 퍼내며 *파이프라인을 겹칩니다*.
>
> 이렇게 다이를 오버랩(overlap)해서 스케줄링하여 IOPS를 높입니다.
